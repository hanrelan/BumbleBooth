# Stable Diffusion using BumbleBee

```elixir
# This should be set appropriately for the system
# eg. cuda118 for a machine with a GPU and CUDA 11.8+
IO.puts(System.get_env("XLA_TARGET"))

Mix.install(
  [
    {:bumblebee, github: "elixir-nx/bumblebee", branch: "main", override: true},
    {:exla, ">= 0.0.0"},
    {:kino_bumblebee, "~> 0.1.0"}
  ],
  config: [nx: [default_backend: EXLA.Backend]]
)

Nx.global_default_backend(EXLA.Backend)
Nx.Defn.global_default_options(compiler: EXLA)
```

## Run SD inference with Nx.Serving

To start, we're going to try to get Stable Diffusion running using `BumbleBee` and `Nx.Serving`. This should (!) be pretty straightforward, we'll use the example notebook [here](https://github.com/elixir-nx/bumblebee/blob/main/notebooks/stable_diffusion.livemd)

```elixir
repository_id = "CompVis/stable-diffusion-v1-4"

{:ok, tokenizer} = Bumblebee.load_tokenizer({:hf, "openai/clip-vit-large-patch14"})

{:ok, clip} = Bumblebee.load_model({:hf, repository_id, subdir: "text_encoder"})

{:ok, unet} =
  Bumblebee.load_model({:hf, repository_id, subdir: "unet"},
    params_filename: "diffusion_pytorch_model.bin"
  )

{:ok, vae} =
  Bumblebee.load_model({:hf, repository_id, subdir: "vae"},
    architecture: :decoder,
    params_filename: "diffusion_pytorch_model.bin"
  )

{:ok, scheduler} = Bumblebee.load_scheduler({:hf, repository_id, subdir: "scheduler"})
{:ok, featurizer} = Bumblebee.load_featurizer({:hf, repository_id, subdir: "feature_extractor"})
{:ok, safety_checker} = Bumblebee.load_model({:hf, repository_id, subdir: "safety_checker"})

:ok
```

```elixir
serving =
  Bumblebee.Diffusion.StableDiffusion.text_to_image(clip, unet, vae, tokenizer, scheduler,
    num_steps: 20,
    num_images_per_prompt: 2,
    safety_checker: safety_checker,
    safety_checker_featurizer: featurizer,
    compile: [batch_size: 1, sequence_length: 60],
    defn_options: [compiler: EXLA]
  )

text_input =
  Kino.Input.text("Prompt", default: "numbat, forest, high quality, detailed, digital art")
```

```elixir
prompt = Kino.Input.read(text_input)

output = Nx.Serving.run(serving, prompt)

for result <- output.results do
  Kino.Image.new(result.image)
end
|> Kino.Layout.grid(columns: 2)
```

That worked! If you got a CuDNN error in the last step (as I did), check your `XLA_TARGET` and make sure you haven't used one with too high a CuDNN. If so, downgrade your `XLA_TARGET` or upgrade CuDNN.

## SD Inference broken down

Now that we know the basics are working (which is a huge step since it means CUDA, XLA and Nx are all set up correctly), we're going to go a little deeper. Right now, a lot of the details of Stable Diffusion are hidden behind the `Bumblebee.Diffusion.StableDiffusion.text_to_image` function. We're going to break this function down into its parts in this notebook so we can start modifying the pieces in the next step.

For this breakdown, we're going to ignore some of the unnecessary/less interesting bits like the safety checker and doing multiple images per prompt.

This breakdown is based on the code underlying [Bumblebee.Diffusion.StableDiffusion.text_to_image](https://github.com/elixir-nx/bumblebee/blob/main/lib/bumblebee/diffusion/stable_diffusion.ex) and [this notebook](https://github.com/fastai/diffusion-nbs/blob/master/Stable%20Diffusion%20Deep%20Dive.ipynb) from [fast.ai](https://fast.ai)

Let's start by processing the prompt - in this case processing means turn the natural language text into text embeddings.

The way SD works is we create outputs for two prompts: the conditional prompt which is the prompt we give it, and the unconditional prompt which is simply the empty string. The final result is a sort of weighted sum between these two results.

```elixir
prompt = Kino.Input.read(text_input)
seq_length = 60
batch_size = 2
num_steps = 20
guidance_scale = 7.5

tokenizer_options = [
  length: seq_length,
  return_token_type_ids: false,
  return_attention_mask: false
]

cond_tokens = Bumblebee.Text.ClipTokenizer.apply(tokenizer, prompt, tokenizer_options)
uncond_tokens = Bumblebee.Text.ClipTokenizer.apply(tokenizer, "", tokenizer_options)
# Since cond_tokens and uncond_tokens are maps, this concats the corresponding keys correctly
tokens = Bumblebee.Utils.Nx.composite_concatenate(uncond_tokens, cond_tokens)
%{hidden_state: text_embeddings} = Axon.predict(clip.model, clip.params, tokens)
# Shape = {2, 60, 768}
text_embeddings
```

We have the text embeddings for the conditional and unconditional prompt, but we need to replicate these to match our batch size. We're going to put our batch size in the *2nd* dimension so later we can easily split the output into the conditional and unconditional parts.

```elixir
text_embeddings =
  text_embeddings
  |> Nx.new_axis(1)
  |> Nx.tile([1, batch_size, 1, 1])
  |> Nx.reshape({:auto, seq_length, 768})
```

The final shape of `text_embeddings` is `{batch_size*2, 60, 768}` with the first batch of size batch_size being the embeddings for the empty prompt (unconditional) and the 2nd batch being the embeddings for our target prompt (conditional).

Next, we'll create our starting random latent vectors, one for each generation we're going to do. For this model, we can look at the spec to determine these are 64x64x4 tensors. So our final output is going to have shape `{batch_size, 64, 64, 4}`, one latent for each image we're going to generate.

```elixir
latents_shape = {batch_size, unet.spec.sample_size, unet.spec.sample_size, unet.spec.in_channels}
key = Nx.Random.key(0)
{latents, _new_key} = Nx.Random.normal(key, shape: latents_shape)
latents
```

Now we have all the pieces, we can do a single step of our eventual loop. We'll initialize the scheduler and then predict the in our current (totally noisy) latents. We can even try to visualize our outputs - though don't expect much... yet.

```elixir
defmodule SDHelper do
  import Nx.Defn

  # We need this in a module so we can use use `Nx.Defn` so the operators work
  defn apply_guidance(noise_pred_uncond, noise_pred_cond, guidance_scale) do
    noise_pred_uncond + guidance_scale * (noise_pred_cond - noise_pred_uncond)
  end

  # TODO: remove this once https://github.com/elixir-nx/bumblebee/issues/123 lands in main
  defn scheduler_step(step_fn, scheduler_state, latents, noise_pred) do
    step_fn.(scheduler_state, latents, noise_pred)
  end
end

{scheduler_state, timesteps} = Bumblebee.scheduler_init(scheduler, num_steps, latents_shape)

unet_inputs = %{
  # One batch for uncond and one for cond_tokens
  "sample" => Nx.concatenate([latents, latents]),
  "timestep" => timesteps[0],
  "encoder_hidden_state" => text_embeddings
}

%{sample: noise_pred} = Axon.predict(unet.model, unet.params, unet_inputs)
noise_pred_uncond = noise_pred[0..(batch_size - 1)]
noise_pred_cond = noise_pred[batch_size..-1//1]
noise_pred = SDHelper.apply_guidance(noise_pred_uncond, noise_pred_cond, guidance_scale)
scheduler_step_fn = &Bumblebee.scheduler_step(scheduler, &1, &2, &3)

{_state, new_latents} =
  SDHelper.scheduler_step(scheduler_step_fn, scheduler_state, latents, noise_pred)
```

Now that we have our new latents after one step of the diffusion process, we can run them through the VAE to see what the image looks like. But we're not expecting much since it's just a single step.

```elixir
# Scaling before we pass it to the VAE
new_latents = Nx.multiply(new_latents, 1 / 0.18215)
%{sample: image} = Axon.predict(vae.model, vae.params, new_latents)
images = NxImage.from_continuous(image, -1, 1)

Kino.Layout.grid(
  [
    Kino.Image.new(images[0]),
    Kino.Image.new(images[1])
  ],
  boxed: true,
  columns: 2
)
```

It's a brown mess but that's what we expected. Let's put this into a loop and run all 20 steps so we can get a real image. We'll use a `while` loop in `defn` for a performance gain. To do that, we'll have to use the pre-built versions of our models to avoid issues passing them into the `defn`

```elixir
{_, unet_predict} = Axon.build(unet.model, compiler: EXLA)
{_, vae_predict} = Axon.build(vae.model, compiler: EXLA)

defmodule SDLoop do
  import Nx.Defn

  defn run(
         guidance_scale,
         latents,
         timesteps,
         text_embeddings,
         unet_predict,
         unet_params,
         scheduler_step_fn,
         scheduler_state
       ) do
    {_, latents, _, _, _} =
      while {scheduler_state, latents, unet_params, text_embeddings, guidance_scale},
            timestep <- timesteps do
        unet_inputs = %{
          "sample" => Nx.concatenate([latents, latents]),
          "timestep" => timestep,
          "encoder_hidden_state" => text_embeddings
        }

        %{sample: noise_pred} = unet_predict.(unet_params, unet_inputs)
        batch_size = div(Nx.axis_size(noise_pred, 0), 2)
        noise_pred_uncond = noise_pred[0..(batch_size - 1)]
        noise_pred_cond = noise_pred[batch_size..-1//1]
        noise_pred = SDHelper.apply_guidance(noise_pred_uncond, noise_pred_cond, guidance_scale)

        {scheduler_state, latents} =
          SDHelper.scheduler_step(scheduler_step_fn, scheduler_state, latents, noise_pred)

        {scheduler_state, latents, unet_params, text_embeddings, guidance_scale}
      end

    latents
  end
end

new_latents =
  SDLoop.run(
    guidance_scale,
    latents,
    timesteps,
    text_embeddings,
    unet_predict,
    unet.params,
    scheduler_step_fn,
    scheduler_state
  )
```

```elixir
# Scaling before we pass it to the VAE
new_latents = Nx.multiply(new_latents, 1 / 0.18215)
%{sample: image} = Axon.predict(vae.model, vae.params, new_latents)
images = NxImage.from_continuous(image, -1, 1)

Kino.Layout.grid(
  [
    Kino.Image.new(images[0]),
    Kino.Image.new(images[1])
  ],
  boxed: true,
  columns: 2
)
```
